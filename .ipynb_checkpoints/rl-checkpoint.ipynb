{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from maxsum import check_validity, get_pairing_matrix_argmax, reshape_to_square\n",
    "from maxsum_ul import get_datasets, forward_pass, decompose_dataset, FILENAMES, N_DATASET, N_NODE, SEED_W\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "GAMMA = 1.0\n",
    "N_TRAIN = 9000\n",
    "N_TEST = 1\n",
    "N_EPISODE = 10000\n",
    "MAX_TIMESTEP = 10\n",
    "FILENAMES[\"nn_weight_alpha\"] = \"weights_rl_alpha.h5\"\n",
    "FILENAMES[\"nn_weight_rho\"] = \"weights_rl_rho.h5\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    pos_bs, pos_user, w, alpha_star, rho_star = get_datasets(\n",
    "        FILENAMES, N_DATASET, SEED_W, N_NODE, \"geographic\")\n",
    "    dim_in = 2*N_NODE**2\n",
    "    dim_out = N_NODE**2\n",
    "    learning_rate = 5e-5\n",
    "    pi_alpha = Pi(dim_in, dim_out)\n",
    "    pi_rho = Pi(dim_in, dim_out)\n",
    "    optim_alpha = optim.Adam(pi_alpha.parameters(), lr=learning_rate)\n",
    "    optim_rho = optim.Adam(pi_rho.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_history = []\n",
    "    reward_history = []\n",
    "    solved_history = []\n",
    "    loss_fn = nn.MSELoss(reduction='sum')\n",
    "    for data_no in range(N_TRAIN):\n",
    "        print(f\"Train set {data_no} weights:\\n{reshape_to_square(w[data_no], N_NODE)}\")\n",
    "        plot_positions(pos_bs[data_no], pos_user[data_no], data_no)\n",
    "        D_mp = get_pairing_matrix_argmax(reshape_to_square(alpha_star[data_no], N_NODE),\n",
    "                                         reshape_to_square(rho_star[data_no], N_NODE),\n",
    "                                         N_NODE)\n",
    "        print(f\"D(mp):\\n{D_mp}\")\n",
    "        init_alpha = torch.zeros((N_NODE**2))\n",
    "        init_rho = torch.zeros((N_NODE**2))\n",
    "        w_tensor = torch.from_numpy(w[data_no].astype(np.float32))\n",
    "\n",
    "        # init_alpha, init_rho = cheat_start(init_alpha, init_rho,\n",
    "        #                                     w[data_no], n_cheat_step=10)\n",
    "\n",
    "        for epi_no in range(N_EPISODE):\n",
    "            alpha = init_alpha\n",
    "            rho = init_rho\n",
    "            alpha_hist = np.zeros((MAX_TIMESTEP, N_NODE**2))\n",
    "            rho_hist = np.zeros((MAX_TIMESTEP, N_NODE**2))\n",
    "            alpha_target_hist = np.zeros((MAX_TIMESTEP, N_NODE**2))\n",
    "            rho_target_hist = np.zeros((MAX_TIMESTEP, N_NODE**2))\n",
    "            alpha_hist[0] = alpha.detach().numpy()\n",
    "            rho_hist[0] = rho.detach().numpy()\n",
    "            alpha_target_hist[0] = alpha.detach().numpy()\n",
    "            rho_target_hist[0] = rho.detach().numpy()\n",
    "            \n",
    "            for t in range(1, MAX_TIMESTEP):\n",
    "                # print(f\"====timestep: {t}====\")\n",
    "                # print(f\"Prev. alpha: \\n{reshape_to_square(alpha.detach().numpy(), N_NODE)}\")\n",
    "                # print(f\"Prev rho: \\n{reshape_to_square(rho.detach().numpy(), N_NODE)}\")\n",
    "                alpha_rho = torch.cat((alpha, rho)).detach().numpy().reshape(1, -1)\n",
    "                alpha_rho_target = forward_pass(w[data_no], alpha_rho)\n",
    "                alpha_target, rho_target = np.array_split(alpha_rho_target, 2)\n",
    "                alpha_target = torch.from_numpy(alpha_target.astype(np.float32))\n",
    "                rho_target = torch.from_numpy(rho_target.astype(np.float32))\n",
    "\n",
    "                alpha_target_hist[t] = alpha_target.detach().numpy()\n",
    "                rho_target_hist[t] = rho_target.detach().numpy()\n",
    "\n",
    "                action_alpha = pi_alpha.act(torch.cat((rho, w_tensor)))\n",
    "                action_rho = pi_rho.act(torch.cat((alpha, w_tensor)))\n",
    "                # print(f\"Action (alpha): \\n{reshape_to_square(action_alpha.detach().numpy(), N_NODE)}\")\n",
    "                # print(f\"Action (rho): \\n{reshape_to_square(action_rho.detach().numpy(), N_NODE)}\")\n",
    "                alpha = alpha + action_alpha\n",
    "                rho = rho + action_rho\n",
    "\n",
    "                # print(f\"New alpha: \\n{reshape_to_square(alpha.detach().numpy(), N_NODE)}\")\n",
    "                # print(f\"New rho: \\n{reshape_to_square(rho.detach().numpy(), N_NODE)}\")\n",
    "\n",
    "\n",
    "                # n_up_alpha = 0\n",
    "                # n_down_alpha = 0\n",
    "                # n_up_rho = 0\n",
    "                # n_down_rho = 0\n",
    "                # if (torch.max(alpha) > 1 or torch.min(alpha) < -1):\n",
    "                #     n_up_alpha = torch.count_nonzero(alpha > 1)\n",
    "                #     n_down_alpha = torch.count_nonzero(alpha < -1)\n",
    "                #     reward_alpha -= 10 * \\\n",
    "                #         (n_up_alpha.item() + n_down_alpha.item())\n",
    "                #     alpha = torch.clamp(alpha, -1, 1)\n",
    "                # if (torch.max(rho) > 1 or torch.min(rho) < -1):\n",
    "                #     n_up_rho = torch.count_nonzero(rho > 1)\n",
    "                #     n_down_rho = torch.count_nonzero(rho < -1)\n",
    "                #     reward_rho -= 10*(n_up_rho.item() + n_down_rho.item())\n",
    "                #     rho = torch.clamp(rho, -1, 1)\n",
    "                reward_alpha = -loss_fn(alpha, alpha_target)\n",
    "                reward_rho = -loss_fn(rho, rho_target)\n",
    "                # print(f\"Reward (rho): {reward_rho}\")\n",
    "                pi_alpha.rewards.append(reward_alpha)\n",
    "                pi_rho.rewards.append(reward_rho)\n",
    "\n",
    "                alpha_hist[t] = alpha.detach().numpy()\n",
    "                rho_hist[t] = rho.detach().numpy()\n",
    "\n",
    "                alpha = alpha.detach()\n",
    "                rho = rho.detach()\n",
    "\n",
    "            loss_sum_alpha = train(pi_alpha, optim_alpha, True)\n",
    "            loss_sum_rho = train(pi_rho, optim_rho, False)\n",
    "            \n",
    "            D_rl = get_pairing_matrix_argmax(\n",
    "                reshape_to_square(alpha.detach().numpy(), N_NODE),\n",
    "                reshape_to_square(rho.detach().numpy(), N_NODE), N_NODE)\n",
    "            solved_history.append((D_rl==D_mp).all())\n",
    "            loss_history.append(loss_sum_alpha+loss_sum_rho)\n",
    "            total_reward_alpha = sum(pi_alpha.rewards)\n",
    "            total_reward_rho = sum(pi_rho.rewards)\n",
    "            reward_history.append(total_reward_alpha + total_reward_rho)\n",
    "\n",
    "            if epi_no % 1000 == 0:\n",
    "                print(f\"(Dataset{data_no} episode{epi_no}) \"\n",
    "                    f\"total_reward: {total_reward_alpha+total_reward_rho:.2f}, \"\n",
    "                    f\"sum_log_prob: {np.sum(pi_alpha.log_probs) + np.sum(pi_rho.log_probs):.2f}, \"\n",
    "                    f\"loss: {loss_sum_alpha+loss_sum_rho:.2f}, \"\n",
    "                    f\"D_rl==D_mp: {(D_rl==D_mp).all()}\")\n",
    "                compare_with_target(alpha_hist, alpha_target_hist, rho_hist, rho_target_hist,\n",
    "                                    data_no, epi_no)\n",
    "            pi_alpha.onpolicy_reset()\n",
    "            pi_rho.onpolicy_reset()\n",
    "    torch.save(pi_alpha.model.state_dict(), FILENAMES[\"nn_weight_alpha\"])\n",
    "    torch.save(pi_rho.model.state_dict(), FILENAMES[\"nn_weight_rho\"])\n",
    "    plot_reward_loss_vs_epoch(\n",
    "        reward_history, loss_history, solved_history, data_no)\n",
    "\n",
    "\n",
    "class Pi(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(Pi, self).__init__()\n",
    "        layers=[\n",
    "            nn.Linear(dim_in, 70),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(70, 50),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(50, 30),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(30, dim_out),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        self.model=nn.Sequential(*layers)\n",
    "        self.onpolicy_reset()\n",
    "        self.train()  # set training mode\n",
    "\n",
    "    def onpolicy_reset(self):\n",
    "        self.log_probs=[]\n",
    "        self.rewards=[]\n",
    "\n",
    "    def act(self, state):\n",
    "        action = self.model(state)\n",
    "        return action\n",
    "\n",
    "\n",
    "def cheat_start(init_alpha, init_rho, w, n_cheat_step):\n",
    "    for n in range(n_cheat_step):\n",
    "        init_alpha_rho = (torch.cat((init_alpha, init_rho))).detach().numpy().reshape(1,-1)\n",
    "        init_alpha_rho = forward_pass(w, init_alpha_rho)\n",
    "        init_alpha, init_rho = np.array_split(init_alpha_rho, 2)\n",
    "        init_alpha = torch.from_numpy(init_alpha.astype(np.float32))\n",
    "        init_rho = torch.from_numpy(init_rho.astype(np.float32))\n",
    "    return init_alpha, init_rho\n",
    "\n",
    "\n",
    "def train(pi, optimizer, bRetainGraph):\n",
    "    T = len(pi.rewards)\n",
    "    returns = torch.zeros(T)\n",
    "    future_returns = 0.0\n",
    "    for t in reversed(range(T)):\n",
    "        future_returns = pi.rewards[t] + GAMMA * future_returns\n",
    "        returns[t] = future_returns\n",
    "    loss = -returns\n",
    "    loss = torch.sum(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=bRetainGraph)\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compare_pairings(D_rl, D_mp):\n",
    "    diff_count=int(np.sum(abs(D_rl - D_mp))/2)\n",
    "    print(f\"{diff_count} pairings are different than the optimum.\")\n",
    "\n",
    "\n",
    "def plot_positions(bs, user, idx):\n",
    "    _, ax = plt.subplots()\n",
    "    ax.set_title(\"BS and user positions\")\n",
    "    ax.set_xlabel(\"x [m]\")\n",
    "    ax.set_ylabel(\"y [m]\")\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    for i in range(N_NODE):\n",
    "        ax.plot(bs[i, 0], bs[i, 1], 'b',\n",
    "                marker=f\"$b{i}$\", markersize=16)\n",
    "        ax.plot(user[i, 0], user[i, 1], 'r',\n",
    "                marker=f\"$u{i}$\", markersize=16)\n",
    "    plt.savefig(f\"training/dataset{idx}_pos.png\")\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "def plot_reward_loss_vs_epoch(reward_history,\n",
    "                              loss_history,\n",
    "                              solved_history, idx):\n",
    "    _, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n",
    "    axes[0].set_title('reward')\n",
    "    axes[1].set_title('loss')\n",
    "    epi = np.linspace(0, len(reward_history)-1, len(reward_history))\n",
    "\n",
    "    reward_history_solved = np.asfarray(reward_history)\n",
    "    reward_history_solved[[not solved for solved in solved_history]] = np.nan\n",
    "    reward_history_unsolved = np.asfarray(reward_history)\n",
    "    reward_history_unsolved[solved_history] = np.nan\n",
    "    axes[0].plot(epi, reward_history_solved,\n",
    "                 \"*\", color='green', markersize=1)\n",
    "    axes[0].plot(epi, reward_history_unsolved,\n",
    "                 \"o\", color='red', markersize=1)\n",
    "    axes[0].plot(epi, reward_history,\n",
    "                 \"-\", color='black', alpha=0.2)\n",
    "    axes[0].set_xlim(xmin=0)\n",
    "\n",
    "    loss_history_solved = np.asfarray(loss_history)\n",
    "    loss_history_solved[[not solved for solved in solved_history]]=np.nan\n",
    "    loss_history_unsolved=np.asfarray(loss_history)\n",
    "    loss_history_unsolved[solved_history]=np.nan\n",
    "    axes[1].plot(epi, loss_history_solved,\n",
    "                 \"o\", color='green', markersize=2)\n",
    "    axes[1].plot(epi, loss_history_unsolved,\n",
    "                 \"o\", color='red', markersize=2)\n",
    "    axes[1].plot(epi, loss_history,\n",
    "                 \"-\", color='black', alpha=0.2)\n",
    "    axes[1].set_xlim(xmin=0)\n",
    "    plt.savefig(f\"training/dataset{idx}.png\")\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "def compare_with_target(alpha_hist, alpha_target_hist, rho_hist, rho_target_hist, idx, episode):\n",
    "    _, axes = plt.subplots(nrows=N_NODE, ncols=2, figsize=(12,24))\n",
    "    axes[0, 0].set_title(\"alpha\")\n",
    "    axes[0, 1].set_title(\"rho\")\n",
    "    t = np.linspace(0, MAX_TIMESTEP-1, MAX_TIMESTEP)\n",
    "    marker = itertools.cycle((\"$1$\", \"$2$\", \"$3$\", \"$4$\", \"$5$\"))\n",
    "    for i in range(N_NODE):\n",
    "        for j in range (N_NODE):\n",
    "            mk = next(marker)\n",
    "            axes[i, 0].plot(t, alpha_hist[:, N_NODE*i+j],\n",
    "                        marker=mk,\n",
    "                        color='green')\n",
    "            axes[i, 0].plot(t, alpha_target_hist[:, N_NODE*i+j],\n",
    "                        marker=mk,\n",
    "                        color='red', linewidth=0)\n",
    "            axes[i, 1].plot(t, rho_hist[:, N_NODE*i+j],\n",
    "                            marker=mk,\n",
    "                            color='green')\n",
    "            axes[i, 1].plot(t, rho_target_hist[:, N_NODE*i+j],\n",
    "                            marker=mk,\n",
    "                            color='red', linewidth=0)\n",
    "        axes[i, 0].set_xlim(xmin=0, xmax=MAX_TIMESTEP-1)\n",
    "        axes[i, 1].set_xlim(xmin=0, xmax=MAX_TIMESTEP-1)\n",
    "        # axes[i, 0].set_ylim(ymin=-3, ymax=3)\n",
    "        # axes[i, 1].set_ylim(ymin=-3, ymax=3)\n",
    "    # axes[0].legend()\n",
    "    # axes[1].legend()\n",
    "    plt.savefig(f\"training/dataset{idx}_epi{episode}.png\")\n",
    "    plt.close('all')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
